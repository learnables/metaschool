{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"changelog/","text":"Changelog \u00b6 All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . [Unreleased] \u00b6 Added \u00b6 Changed \u00b6 Fixed \u00b6","title":"Changelog"},{"location":"changelog/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"changelog/#unreleased","text":"","title":"[Unreleased]"},{"location":"changelog/#added","text":"","title":"Added"},{"location":"changelog/#changed","text":"","title":"Changed"},{"location":"changelog/#fixed","text":"","title":"Fixed"},{"location":"api/","text":"metaschool \u00b6 Base Classes \u00b6 metaschool.TaskConfig \u00b6 [Source] Description \u00b6 Dictionary-like object to store task and wrapper configurations. Example \u00b6 config = TaskConfig ( seed = 42 , car = 'Volvo' ) config . location = 'Town2' config . weather = 'Cloudy' metaschool.EnvFactory \u00b6 [Source] Description \u00b6 Base class to define how a task is created, and how its configuration is sampled. Task creation proceeds in two steps: Sampling of the random parameters of the task (see sample() ). Instantiation of the task given chosen parameters (see make() ). In a single-task setting, this is typically done with one large function (typically named make_env ). Example \u00b6 class DrivingFactory ( EnvFactory ): def make ( self , config ): env = gym . make ( 'DrivingEnv-v0' , ** config ) env = gym . wrappers . RecordEpisodeStatistics ( env ) return env def sample ( self ): config = TaskConfig ( location = 'Town2' , weather = 'Sunny' ) config . color = random . choice ([ 'Volvo' , 'Mercedes' , 'Audi' , 'Hummer' ]) return config factory = DrivingFactory () train_task = factory . make ( factory . sample ()) test_task = factory . make ( factory . sample ()) make ( self , config : TaskConfig ) -> Env \u00b6 Description \u00b6 Defines how to create a task, given a configuration. Use this method to wrap your base environment with the default wrappers. It's important that this method be deterministic when given the same configuration twice -- this allows to generate identical copies of the same task. Arguments \u00b6 config (TaskConfig) - Configuration of the task. Returns \u00b6 env (Env) - The gym environment defined according to config. sample ( self ) -> TaskConfig \u00b6 Description \u00b6 Defines the sampling procedure over the task parameters. Use this method to choose the axes of variation across tasks. For example, the above snippet ensures that the driving environment will always be in Town2 with a sunny weather; the car make is randomized. Returns \u00b6 config (TaskConfig) - A randomly sampled task configuration for this environment. metaschool.WrapperFactory \u00b6 [Source] Description \u00b6 Base class to define how a task is created, and how its configuration is sampled. It is useful when some axes of variation across tasks are more easily implemented through wrappers around a gym.Env . It also allows for re-use of wrapper factories across projects. Example \u00b6 class ChangingHorizonFactory ( EnvFactory ): min_horizon = 20 max_horizon = 200 def wrap ( self , env , config ): return gym . wrappers . TimeLimit ( env , max_episode_steps = config . max_steps ) def sample ( self , env = None ): return TaskConfig ( max_steps = random . randint ( self . min_horizon , self . max_horizon )) wrapper_factory = ChangingHorizonFactory () wrapper_config = wrapper_config . sample () task = wrapper_factory . wrap ( task , wrapper_config ) sample ( self , env : Env = None ) -> TaskConfig \u00b6 Description \u00b6 Samples a parameter configuration for the current wrapper. Arguments \u00b6 env (Env, optional , default=None) - The environment to be wrapped. Returns \u00b6 config (TaskConfig) - The sampled parameter configuration. wrap ( self , env : Env , config : TaskConfig ) -> Env \u00b6 Description \u00b6 Wraps a gym environment according to the parameters defined in config. Arguments \u00b6 env (Env) - The environment to wrap. config (TaskConfig) - The configuration of the wrapper parameters. Returns \u00b6 env (Env) - The wrapped environment. Utility Classes \u00b6 metaschool.GymTaskset \u00b6 [Source] Description \u00b6 A utility class to easily sample tasks and keeping track of previously seen tasks. Example \u00b6 taskset = GymTaskset ( env_factory = MyEnvFactory (), wrapper_factories = [ MyWrapperFactory1 (), MyWrapperFactory2 (), MyWrapperFactory3 (), ], ) for iteration in range ( num_iterations ): train_task = taskset . sample () # ... train on train_task ... test_task = taskset . make_like ( train_task ) # ... test on freshly generated (and identical) test_task ... # enumerate sampled tasks for seen_task in iter ( taskset ): # ... process previously seen task ... __init__ ( self , env_factory , wrapper_factories = None ) \u00b6 Arguments \u00b6 env_factory (EnvFactory) - An environment factory. wrapper_factories (WrapperFactory, optional , default=None) - A list of wrapper factories. make_like ( self , env : Env ) -> Env \u00b6 Description \u00b6 Given a previously sampled environment, instantiates a new copy based on its sampled (env and wrapper) configurations. Useful to have identical environment instances for training and testing. Arguments \u00b6 env (Env) - The environment to copy. Returns \u00b6 env (Env) - The newly created copy. sample ( self ) -> Env \u00b6 Description \u00b6 Samples environment and wrapper configurations, and returns the instantiated wrapped environment. Returns \u00b6 env (Env) - The wrapped environment with newly sampled configurations.","title":"metaschool"},{"location":"api/#metaschool","text":"","title":"metaschool"},{"location":"api/#base-classes","text":"","title":"Base Classes"},{"location":"api/#metaschool.TaskConfig","text":"[Source]","title":"TaskConfig"},{"location":"api/#metaschool.EnvFactory","text":"[Source]","title":"EnvFactory"},{"location":"api/#metaschool.EnvFactory.make","text":"","title":"make()"},{"location":"api/#metaschool.EnvFactory.sample","text":"","title":"sample()"},{"location":"api/#metaschool.WrapperFactory","text":"[Source]","title":"WrapperFactory"},{"location":"api/#metaschool.WrapperFactory.sample","text":"","title":"sample()"},{"location":"api/#metaschool.WrapperFactory.wrap","text":"","title":"wrap()"},{"location":"api/#utility-classes","text":"","title":"Utility Classes"},{"location":"api/#metaschool.GymTaskset","text":"[Source]","title":"GymTaskset"},{"location":"api/#metaschool.GymTaskset.__init__","text":"","title":"__init__()"},{"location":"api/#metaschool.GymTaskset.make_like","text":"","title":"make_like()"},{"location":"api/#metaschool.GymTaskset.sample","text":"","title":"sample()"},{"location":"api/metaschool.envs/","text":"metaschool.envs \u00b6 OpenAI Gym environments included with metaschool. metaschool.envs.MSRJumpEnv \u00b6 [Source] Description \u00b6 Bare bone re-implementation of the MSR Montreal Jumping task. References \u00b6 Tachet des Combes et al. 2018. \"Learning Invariances for Policy Generalization\" ICLR 2018 Workshop Track Example \u00b6 env = MSRJumpEnv ( screen_width = 64 , screen_height = 64 , obstacle_position = 25 , agent_speed = 2 , jumping_height = 15 , obstacle_size = ( 10 , 10 ), agent_size = ( 10 , 10 ), ) env . reset () while True : action = env . get_optimal_action () s , r , d , _ = env . step ( action ) if d : break print ( 'Max achievable reward:' , env . reward_range [ 1 ]) __init__ ( self , floor_height = 10 , obstacle_position = 20 , obstacle_size = ( 9 , 10 ), vision_observations = True , screen_width = 84 , screen_height = 84 , jumping_height = 15 , agent_speed = 1 , agent_size = ( 5 , 10 ), colors = None , device = None , num_envs = 1 , max_episode_steps = 600 ) \u00b6 Arguments \u00b6 floor_height (int, optional , default=10) - Height of the floor. obstacle_position (int, optional , default=20) - X-position of the obstacle. obstacle_size (tuple, optional , default=(9, 10)) - Dimensions of obstacle. vision_observations (bool, optional , default=True) - Use vision observations or physical state. screen_width (int, optional , default=84) - Witdth of the screen. screen_height (int, optional , default=84) - Height of the screen jumping_height (int, optional , default=15) - Height of agent's jump. agent_speed (int, optional , default=1 - Speed of agent. agent_size (tuple, optional , default=(5, 10)) - Dimensions of agent. colors (class, optional , default=None) - Color values for visual observations. device (torch.device, optional , default=None) - Device for observation tensors. num_envs (int, optional , default=1) - Number of parallel environments (unsupported). max_episode_steps (int, optional , default=600) - Horizon length. get_optimal_action ( self ) \u00b6 Returns the optimal action given current state of the world.","title":"metaschool.envs"},{"location":"api/metaschool.envs/#metaschoolenvs","text":"OpenAI Gym environments included with metaschool.","title":"metaschool.envs"},{"location":"api/metaschool.envs/#metaschool.envs.MSRJumpEnv","text":"[Source]","title":"MSRJumpEnv"},{"location":"api/metaschool.envs/#metaschool.envs.MSRJumpEnv.__init__","text":"","title":"__init__()"},{"location":"api/metaschool.envs/#metaschool.envs.MSRJumpEnv.get_optimal_action","text":"Returns the optimal action given current state of the world.","title":"get_optimal_action()"},{"location":"api/metaschool.tasks/","text":"metaschool.tasks \u00b6 Standardized tasks definitions from the literature.","title":"metaschool.tasks"},{"location":"api/metaschool.tasks/#metaschooltasks","text":"Standardized tasks definitions from the literature.","title":"metaschool.tasks"}]}