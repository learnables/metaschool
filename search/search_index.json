{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Template to start new PyPI packages To get started, grep and replace for mypackage everywhere.","title":"Home"},{"location":"changelog/","text":"Changelog \u00b6 All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . [Unreleased] \u00b6 Added \u00b6 Changed \u00b6 Fixed \u00b6","title":"Changelog"},{"location":"changelog/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"changelog/#unreleased","text":"","title":"[Unreleased]"},{"location":"changelog/#added","text":"","title":"Added"},{"location":"changelog/#changed","text":"","title":"Changed"},{"location":"changelog/#fixed","text":"","title":"Fixed"},{"location":"api/","text":"mypackage \u00b6","title":"metaschool"},{"location":"api/#mypackage","text":"","title":"mypackage"},{"location":"api/utils/","text":".katex img { object-fit: fill; padding: unset; display: block; position: absolute; width: 100%; height: inherit; } mypackage.utils \u00b6 Equation from Markdown: f ( x ) = \u222b \u2212 \u221e \u221e f ^ ( \u03be ) \u2009 e 2 \u03c0 i \u03be x \u2009 d \u03be f(x) = \\int_{-\\infty}^\\infty \\hat f(\\xi)\\,e^{2 \\pi i \\xi x} \\,d\\xi f ( x ) = \u222b \u2212 \u221e \u221e \u200b f ^ \u200b ( \u03be ) e 2 \u03c0i \u03be x d \u03be mypackage . utils . echo ( msg ) \u00b6 [Source] First, a short description. Throw in an equation, for good measure: \\int_\\Omega f(x) p(x) dx Arguments \u00b6 msg (string): The message to be printed. Example \u00b6 echo ( 'Hello world!' ) mypackage.utils.Example \u00b6 [Source] General wrapper for gradient-based meta-learning implementations. A variety of algorithms can simply be implemented by changing the kind of transform used during fast-adaptation. For example, if the transform is Scale we recover Meta-SGD [2] with adapt_transform=False and Alpha MAML [4] with adapt_transform=True . If the transform is a Kronecker-factored module (e.g. neural network, or linear), we recover KFO from [5]. References \u00b6 Finn et al. 2017. \u201cModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\u201d Li et al. 2017. \u201cMeta-SGD: Learning to Learn Quickly for Few-Shot Learning.\u201d Park & Oliva. 2019. \u201cMeta-Curvature.\u201d Behl et al. 2019. \u201cAlpha MAML: Adaptive Model-Agnostic Meta-Learning.\u201d Arnold et al. 2019. \u201cWhen MAML Can Adapt Fast and How to Assist When It Cannot.\u201d Example \u00b6 model = SmallCNN () transform = l2l . optim . ModuleTransform ( torch . nn . Linear ) gbml = l2l . algorithms . GBML ( module = model , transform = transform , lr = 0.01 , adapt_transform = True , ) __init__ ( self , module , transform , lr = 1.0 , adapt_transform = False , first_order = False , allow_unused = False , allow_nograd = False , ** kwargs ) \u00b6 Arguments \u00b6 module (Module) - Module to be wrapped. tranform (Module) - Transform used to update the module. lr (float) - Fast adaptation learning rate. adapt ( self , loss , first_order = None , allow_nograd = None , allow_unused = None ) \u00b6 Takes a gradient step on the loss and updates the cloned parameters in place. The parameters of the transform are only adapted if self.adapt_update is True . Arguments \u00b6 loss (Tensor) - Loss to minimize upon update. first_order (bool, optional , default=None) - Whether to use first- or second-order updates. Defaults to self.first_order. allow_unused (bool, optional , default=None) - Whether to allow differentiation of unused parameters. Defaults to self.allow_unused. allow_nograd (bool, optional , default=None) - Whether to allow adaptation with parameters that have requires_grad = False . Defaults to self.allow_nograd.","title":"metaschool.utils"},{"location":"api/utils/#mypackageutils","text":"Equation from Markdown: f ( x ) = \u222b \u2212 \u221e \u221e f ^ ( \u03be ) \u2009 e 2 \u03c0 i \u03be x \u2009 d \u03be f(x) = \\int_{-\\infty}^\\infty \\hat f(\\xi)\\,e^{2 \\pi i \\xi x} \\,d\\xi f ( x ) = \u222b \u2212 \u221e \u221e \u200b f ^ \u200b ( \u03be ) e 2 \u03c0i \u03be x d \u03be","title":"mypackage.utils"},{"location":"api/utils/#mypackage.utils.echo","text":"[Source] First, a short description. Throw in an equation, for good measure: \\int_\\Omega f(x) p(x) dx","title":"echo()"},{"location":"api/utils/#mypackage.utils.Example","text":"[Source] General wrapper for gradient-based meta-learning implementations. A variety of algorithms can simply be implemented by changing the kind of transform used during fast-adaptation. For example, if the transform is Scale we recover Meta-SGD [2] with adapt_transform=False and Alpha MAML [4] with adapt_transform=True . If the transform is a Kronecker-factored module (e.g. neural network, or linear), we recover KFO from [5].","title":"Example"},{"location":"api/utils/#mypackage.utils.Example.__init__","text":"","title":"__init__()"},{"location":"api/utils/#mypackage.utils.Example.adapt","text":"Takes a gradient step on the loss and updates the cloned parameters in place. The parameters of the transform are only adapted if self.adapt_update is True .","title":"adapt()"}]}